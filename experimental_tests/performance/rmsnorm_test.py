# Generated by KernelGen v1.0
# Source: Triton
# Performance: 2.6x vs Native on A100
# License: Apache-2.0

import os
import sys
import time

import pytest
import torch

import flag_gems

# Add parent directory to path to import flag_gems
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../../.."))

try:
    from tests.accuracy_utils import FLOAT_DTYPES, gems_assert_close, to_reference
except ImportError:
    # Fallback values when running outside pytest
    FLOAT_DTYPES = [torch.float16, torch.float32, torch.bfloat16]

    def gems_assert_close(res, ref, dtype, **kwargs):
        # Simple fallback comparison
        torch.testing.assert_close(res, ref, **kwargs)

    def to_reference(x, upcast=False):
        # Simple fallback - move to CPU
        return x.to("cpu")


# Use PyTorch native rms_norm as reference
# torch.nn.functional.rms_norm(input, normalized_shape, weight, eps)


# Test shapes for normalization operations
NMSNORM_SHAPES = [
    (16, 512),  # Typical transformer hidden size
    (32, 1024),  # Larger hidden size
    (8, 64, 768),  # Batch, seq_len, hidden_size
    (4, 128, 1024),  # Larger batch and hidden
    (1, 2048),  # Single sample, large hidden
]


@pytest.mark.rmsnorm_performance
@pytest.mark.parametrize("shape", NMSNORM_SHAPES)
@pytest.mark.parametrize("dtype", FLOAT_DTYPES)
def test_nmsnorm_performace(shape, dtype):
    inp = torch.randn(shape, dtype=dtype, device=flag_gems.device)
    # Keep reference on GPU for fair performance comparison
    ref_inp = inp.clone()

    # Test with weight
    weight = torch.randn(shape[-1:], dtype=dtype, device=flag_gems.device)
    ref_weight = weight.clone()

    # Warmup
    for _ in range(10):
        _ = flag_gems.experimental_ops.rmsnorm(inp, weight)

    torch.cuda.synchronize()

    # Benchmark
    start_time = time.time()
    for _ in range(100):
        _ = flag_gems.experimental_ops.rmsnorm(inp, weight)
    torch.cuda.synchronize()
    end_time = time.time()

    gems_time = (end_time - start_time) / 100

    # PyTorch baseline using native rms_norm
    start_time = time.time()
    for _ in range(100):
        _ = torch.nn.functional.rms_norm(
            ref_inp, normalized_shape=[shape[-1]], weight=ref_weight
        )
    torch.cuda.synchronize()
    end_time = time.time()

    torch_time = (end_time - start_time) / 100
    speedup = torch_time / gems_time

    print(f"rmsnorm {shape} {dtype}:")
    print(f"  FlagGems: {gems_time * 1000:.3f}ms")
    print(f"  PyTorch: {torch_time * 1000:.3f}ms")
    print(f"  Speedup: {speedup:.2f}x")
